{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-alarm",
   "metadata": {},
   "source": [
    "## Importing Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unnecessary-pressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "## I recommend the one above, because the following is more accurate but less efficient\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frank-monaco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:25.501688Z",
     "start_time": "2021-03-31T09:15:24.378850Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-usage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:43.994556Z",
     "start_time": "2021-03-28T20:32:43.991087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f8850258430>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f8860afcbe0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f8860afcca0>)]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"print(nlp.pipeline)\";\n",
       "                var nbb_formatted_code = \"print(nlp.pipeline)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marine-rescue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.015363Z",
     "start_time": "2021-03-28T20:32:43.995410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(u\\\"Hello, world. Antonio is learning Python.\\\")\";\n",
       "                var nbb_formatted_code = \"# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\\ndoc = nlp(u\\\"Hello, world. Antonio is learning Python.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-prospect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.021660Z",
     "start_time": "2021-03-28T20:32:44.016337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"for token in doc:\\n    print(token.text)\";\n",
       "                var nbb_formatted_code = \"for token in doc:\\n    print(token.text)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fleet-shooting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.030363Z",
     "start_time": "2021-03-28T20:32:44.022649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_formatted_code = \"# Get first token of the processed document\\ntoken = doc[0]\\nprint(token)\\n\\n# Print sentences (one sentence per line)\\nfor sent in doc.sents:\\n    print(sent)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phantom-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.043811Z",
     "start_time": "2021-03-28T20:32:44.033898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"tokens = nlp(\\\"Let's go to N.Y.!\\\")\";\n",
       "                var nbb_formatted_code = \"tokens = nlp(\\\"Let's go to N.Y.!\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-capitol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.049403Z",
     "start_time": "2021-03-28T20:32:44.045680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"for token in tokens:\\n    print(token.text)\";\n",
       "                var nbb_formatted_code = \"for token in tokens:\\n    print(token.text)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-duplicate",
   "metadata": {},
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accomplished-marker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokenizer.Tokenizer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"tokenizer = nlp.tokenizer\\ntype(tokenizer)\";\n",
       "                var nbb_formatted_code = \"tokenizer = nlp.tokenizer\\ntype(tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-medium",
   "metadata": {},
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olive-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"from spacy.tokenizer import Tokenizer\\n\\ntokenizer = Tokenizer(vocab=nlp.vocab)\";\n",
       "                var nbb_formatted_code = \"from spacy.tokenizer import Tokenizer\\n\\ntokenizer = Tokenizer(vocab=nlp.vocab)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-clerk",
   "metadata": {},
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinct-trade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-county",
   "metadata": {},
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. So we can add rules for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opposed-confusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.076430Z",
     "start_time": "2021-03-28T20:32:44.069248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\\nsuffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.prefixes)\";\n",
       "                var nbb_formatted_code = \"prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\\nsuffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.prefixes)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "foster-highway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.083023Z",
     "start_time": "2021-03-28T20:32:44.077855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"tokenizer = Tokenizer(\\n    vocab=nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\\n)\";\n",
       "                var nbb_formatted_code = \"tokenizer = Tokenizer(\\n    vocab=nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    vocab=nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjacent-organ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.088977Z",
     "start_time": "2021-03-28T20:32:44.083988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-provincial",
   "metadata": {},
   "source": [
    "You can also check the exceptions the tokenizer can handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exposed-compound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.103258Z",
     "start_time": "2021-03-28T20:32:44.092331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'m\", 73: 'be', 67: 'am', 75: 'VBP'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'm', 73: 'be', 75: 'VBP'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'m\", 73: 'be', 67: 'am'}, {65: 'a', 73: 'going to', 67: 'gonna'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'm', 73: 'be', 67: 'am'}, {65: 'a', 73: 'going to', 67: 'gonna'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'m\", 73: 'be', 67: 'am', 75: 'VBP'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'm', 73: 'be', 75: 'VBP'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'m\", 73: 'be', 67: 'am'}, {65: 'a', 73: 'going to', 67: 'gonna'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'm', 73: 'be', 67: 'am'}, {65: 'a', 73: 'going to', 67: 'gonna'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'d\", 67: \"'d\"}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'd', 67: \"'d\"}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'i', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'I', 73: '-PRON-', 67: 'i', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'you', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 're', 73: 'be', 67: 'are', 75: 'VBZ'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'You', 73: '-PRON-', 67: 'you', 75: 'PRP'}, {65: 're', 73: 'be', 67: 'are', 75: 'VBZ'}], [{65: 'we', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'We', 73: '-PRON-', 67: 'we', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'they', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 're', 73: 'be', 67: 'are', 75: 'VBZ'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'They', 73: '-PRON-', 67: 'they', 75: 'PRP'}, {65: 're', 73: 'be', 67: 'are', 75: 'VBZ'}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'he', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 's'}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'He', 73: '-PRON-', 67: 'he', 75: 'PRP'}, {65: 's'}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'she', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 's'}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'She', 73: '-PRON-', 67: 'she', 75: 'PRP'}, {65: 's'}], [{65: 'it', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'It', 73: '-PRON-', 67: 'it', 75: 'PRP'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 73: 'who', 67: 'who'}, {65: 's'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'who', 73: 'who'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'who', 73: 'who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'who', 73: 'who', 67: 'who'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'who', 73: 'who', 67: 'who'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: 's'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Who', 73: 'who', 67: 'who'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 's'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'what', 73: 'what'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'what', 73: 'what', 67: 'what'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'what', 73: 'what', 67: 'what'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 's'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'What', 73: 'what'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'What', 73: 'what', 67: 'what'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'What', 73: 'what', 67: 'what'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 's'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'when', 73: 'when'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'when', 73: 'when', 67: 'when'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'when', 73: 'when', 67: 'when'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 's'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'When', 73: 'when'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'When', 73: 'when', 67: 'when'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'When', 73: 'when', 67: 'when'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 's'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'where', 73: 'where'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'where', 73: 'where', 67: 'where'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'where', 73: 'where', 67: 'where'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 's'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Where', 73: 'where', 67: 'where'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 's'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'why', 73: 'why'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'why', 73: 'why', 67: 'why'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'why', 73: 'why', 67: 'why'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 's'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Why', 73: 'why', 67: 'why'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 's'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'how', 73: 'how'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'how', 73: 'how', 67: 'how'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'how', 73: 'how', 67: 'how'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 's'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'How', 73: 'how'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 's'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'there', 73: 'there'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'there', 73: 'there', 67: 'there'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'there', 73: 'there', 67: 'there'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 's'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'There', 73: 'there'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'There', 73: 'there', 67: 'there'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'There', 73: 'there', 67: 'there'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 's'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'that', 73: 'that'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'that', 73: 'that', 67: 'that'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'that', 73: 'that', 67: 'that'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 's'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'That', 73: 'that'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'That', 73: 'that', 67: 'that'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'That', 73: 'that', 67: 'that'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 's'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'this', 73: 'this'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'this', 73: 'this', 67: 'this'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'this', 73: 'this', 67: 'this'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 's'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'This', 73: 'this'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'This', 73: 'this', 67: 'this'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'This', 73: 'this', 67: 'this'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 's'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'these', 73: 'these'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'these', 73: 'these', 67: 'these'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'these', 73: 'these', 67: 'these'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 's'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'These', 73: 'these'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'These', 73: 'these', 67: 'these'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'These', 73: 'these', 67: 'these'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 's'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'those', 73: 'those'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'those', 73: 'those', 67: 'those'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'those', 73: 'those', 67: 'those'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 's'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'ll\", 73: 'will', 67: 'will', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 'll', 73: 'will', 67: 'will', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'re\", 73: 'be', 67: 'are'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 're', 73: 'be', 67: 'are'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: \"'d\", 73: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Those', 73: 'those', 67: 'those'}, {65: 'd', 73: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Ca', 73: 'can', 67: 'can', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'do', 73: 'do', 67: 'do'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'do', 73: 'do', 67: 'do'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'do', 73: 'do', 67: 'do'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'do', 73: 'do', 67: 'do'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Do', 73: 'do', 67: 'do'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Do', 73: 'do', 67: 'do'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Do', 73: 'do', 67: 'do'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Do', 73: 'do', 67: 'do'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'does', 73: 'do', 67: 'does'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'does', 73: 'do', 67: 'does'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'does', 73: 'do', 67: 'does'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'does', 73: 'do', 67: 'does'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Does', 73: 'do', 67: 'does'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Does', 73: 'do', 67: 'does'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Does', 73: 'do', 67: 'does'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Does', 73: 'do', 67: 'does'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Did', 73: 'do', 67: 'do', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Had', 73: 'have', 67: 'have', 75: 'VBD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'may', 67: 'may', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'may', 67: 'may', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'may', 67: 'may', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'may', 67: 'may', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'May', 67: 'may', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'May', 67: 'may', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'May', 67: 'may', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'May', 67: 'may', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'need', 67: 'need'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'need', 67: 'need'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'ought', 67: 'ought', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'ought', 67: 'ought', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'ought', 67: 'ought', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'ought', 67: 'ought', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Ought', 67: 'ought', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ought', 67: 'ought', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ought', 67: 'ought', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Ought', 67: 'ought', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Sha', 73: 'shall', 67: 'shall', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Wo', 73: 'will', 67: 'will', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'could', 67: 'could', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Could', 67: 'could', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'might', 67: 'might', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Might', 67: 'might', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'must', 67: 'must', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Must', 67: 'must', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'should', 67: 'should', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Should', 67: 'should', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: \"'ve\", 73: 'have', 75: 'VB'}], [{65: 'Would', 67: 'would', 75: 'MD'}, {65: 've', 73: 'have', 75: 'VB'}], [{65: 'ai', 73: 'be', 75: 'VBP'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'ai', 73: 'be', 75: 'VBP'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ai', 73: 'be', 75: 'VBP'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Ai', 73: 'be', 75: 'VBP'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'are', 73: 'be', 67: 'are', 75: 'VBP'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'are', 73: 'be', 67: 'are', 75: 'VBP'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Are', 73: 'be', 67: 'are', 75: 'VBP'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Are', 73: 'be', 67: 'are', 75: 'VBP'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'is', 73: 'be', 67: 'is', 75: 'VBZ'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'is', 73: 'be', 67: 'is', 75: 'VBZ'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Is', 73: 'be', 67: 'is', 75: 'VBZ'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Is', 73: 'be', 67: 'is', 75: 'VBZ'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'was', 73: 'be', 67: 'was'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'was', 73: 'be', 67: 'was'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Was', 73: 'be', 67: 'was'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Was', 73: 'be', 67: 'was'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'were', 73: 'be', 67: 'were'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'were', 73: 'be', 67: 'were'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Were', 73: 'be', 67: 'were'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Were', 73: 'be', 67: 'were'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'have', 67: 'have'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'have', 67: 'have'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Have', 67: 'have'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Have', 67: 'have'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'has', 73: 'have', 67: 'has'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'has', 73: 'have', 67: 'has'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Has', 73: 'have', 67: 'has'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Has', 73: 'have', 67: 'has'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'dare', 67: 'dare'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'dare', 67: 'dare'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Dare', 67: 'dare'}, {65: \"n't\", 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'Dare', 67: 'dare'}, {65: 'nt', 73: 'not', 67: 'not', 75: 'RB'}], [{65: 'doin', 73: 'do', 67: 'doing'}], [{65: \"doin'\", 73: 'do', 67: 'doing'}], [{65: 'Doin', 73: 'do', 67: 'doing'}], [{65: \"Doin'\", 73: 'do', 67: 'doing'}], [{65: 'goin', 73: 'go', 67: 'going'}], [{65: \"goin'\", 73: 'go', 67: 'going'}], [{65: 'Goin', 73: 'go', 67: 'going'}], [{65: \"Goin'\", 73: 'go', 67: 'going'}], [{65: 'nothin', 73: 'nothing', 67: 'nothing'}], [{65: \"nothin'\", 73: 'nothing', 67: 'nothing'}], [{65: 'Nothin', 73: 'nothing', 67: 'nothing'}], [{65: \"Nothin'\", 73: 'nothing', 67: 'nothing'}], [{65: 'nuthin', 73: 'nothing', 67: 'nothing'}], [{65: \"nuthin'\", 73: 'nothing', 67: 'nothing'}], [{65: 'Nuthin', 73: 'nothing', 67: 'nothing'}], [{65: \"Nuthin'\", 73: 'nothing', 67: 'nothing'}], [{65: 'ol', 73: 'old', 67: 'old'}], [{65: \"ol'\", 73: 'old', 67: 'old'}], [{65: 'Ol', 73: 'old', 67: 'old'}], [{65: \"Ol'\", 73: 'old', 67: 'old'}], [{65: 'somethin', 73: 'something', 67: 'something'}], [{65: \"somethin'\", 73: 'something', 67: 'something'}], [{65: 'Somethin', 73: 'something', 67: 'something'}], [{65: \"Somethin'\", 73: 'something', 67: 'something'}], [{65: 'cause', 67: 'because'}], [{65: \"'cause\", 73: 'because', 67: 'because'}], [{65: 'em', 73: '-PRON-', 67: 'them'}], [{65: \"'em\", 73: '-PRON-', 67: 'them'}], [{65: 'll', 73: 'will', 67: 'will'}], [{65: \"'ll\", 73: 'will', 67: 'will'}], [{65: 'nuff', 73: 'enough', 67: 'enough'}], [{65: \"'nuff\", 73: 'enough', 67: 'enough'}], [{65: '1'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '1'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '1'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '1'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '2'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '2'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '2'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '2'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '3'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '3'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '3'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '3'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '4'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '4'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '4'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '4'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '5'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '5'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '5'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '5'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '6'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '6'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '6'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '6'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '7'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '7'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '7'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '7'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '8'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '8'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '8'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '8'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '9'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '9'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '9'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '9'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '10'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '10'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '10'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '10'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '11'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '11'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '11'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '11'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: '12'}, {65: 'a.m.', 73: 'a.m.', 67: 'a.m.'}], [{65: '12'}, {65: 'am', 73: 'a.m.', 67: 'a.m.'}], [{65: '12'}, {65: 'p.m.', 73: 'p.m.', 67: 'p.m.'}], [{65: '12'}, {65: 'pm', 73: 'p.m.', 67: 'p.m.'}], [{65: \"y'\", 73: '-PRON-', 67: 'you'}, {65: 'all'}], [{65: 'y', 73: '-PRON-', 67: 'you'}, {65: 'all'}], [{65: 'how', 73: 'how'}, {65: \"'d\", 73: 'do'}, {65: \"'y\", 73: '-PRON-', 67: 'you'}], [{65: 'How', 73: 'how', 67: 'how'}, {65: \"'d\", 73: 'do'}, {65: \"'y\", 73: '-PRON-', 67: 'you'}], [{65: 'not', 73: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'not', 73: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Not', 73: 'not', 67: 'not', 75: 'RB'}, {65: \"'ve\", 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'Not', 73: 'not', 67: 'not', 75: 'RB'}, {65: 've', 73: 'have', 67: 'have', 75: 'VB'}], [{65: 'can', 73: 'can', 75: 'MD'}, {65: 'not', 73: 'not', 75: 'RB'}], [{65: 'Can', 73: 'can', 67: 'can', 75: 'MD'}, {65: 'not', 73: 'not', 75: 'RB'}], [{65: 'gon', 73: 'go', 67: 'going'}, {65: 'na', 73: 'to', 67: 'to'}], [{65: 'Gon', 73: 'go', 67: 'going'}, {65: 'na', 73: 'to', 67: 'to'}], [{65: 'got'}, {65: 'ta', 73: 'to', 67: 'to'}], [{65: 'Got', 67: 'got'}, {65: 'ta', 73: 'to', 67: 'to'}], [{65: 'let'}, {65: \"'s\", 73: '-PRON-', 67: 'us'}], [{65: 'Let', 73: 'let', 67: 'let'}, {65: \"'s\", 73: '-PRON-', 67: 'us'}], [{65: \"c'm\", 67: 'come', 73: 'come'}, {65: 'on'}], [{65: \"C'm\", 67: 'come', 73: 'come'}, {65: 'on'}], [{65: \"'S\", 73: \"'s\", 67: \"'s\"}], [{65: \"'s\", 73: \"'s\", 67: \"'s\"}], [{65: '‘S', 73: \"'s\", 67: \"'s\"}], [{65: '‘s', 73: \"'s\", 67: \"'s\"}], [{65: 'and/or', 73: 'and/or', 67: 'and/or', 75: 'CC'}], [{65: 'w/o', 73: 'without', 67: 'without'}], [{65: \"'re\", 73: 'be', 67: 'are'}], [{65: \"'Cause\", 73: 'because', 67: 'because'}], [{65: \"'cos\", 73: 'because', 67: 'because'}], [{65: \"'Cos\", 73: 'because', 67: 'because'}], [{65: \"'coz\", 73: 'because', 67: 'because'}], [{65: \"'Coz\", 73: 'because', 67: 'because'}], [{65: \"'cuz\", 73: 'because', 67: 'because'}], [{65: \"'Cuz\", 73: 'because', 67: 'because'}], [{65: \"'bout\", 73: 'about', 67: 'about'}], [{65: \"ma'am\", 73: 'madam', 67: 'madam'}], [{65: \"Ma'am\", 73: 'madam', 67: 'madam'}], [{65: \"o'clock\", 73: \"o'clock\", 67: \"o'clock\"}], [{65: \"O'clock\", 73: \"o'clock\", 67: \"o'clock\"}], [{65: \"lovin'\", 73: 'love', 67: 'loving'}], [{65: \"Lovin'\", 73: 'love', 67: 'loving'}], [{65: 'lovin', 73: 'love', 67: 'loving'}], [{65: 'Lovin', 73: 'love', 67: 'loving'}], [{65: \"havin'\", 73: 'have', 67: 'having'}], [{65: \"Havin'\", 73: 'have', 67: 'having'}], [{65: 'havin', 73: 'have', 67: 'having'}], [{65: 'Havin', 73: 'have', 67: 'having'}], [{65: 'Mt.', 73: 'Mount', 67: 'Mount'}], [{65: 'Ak.', 73: 'Alaska', 67: 'Alaska'}], [{65: 'Ala.', 73: 'Alabama', 67: 'Alabama'}], [{65: 'Apr.', 73: 'April', 67: 'April'}], [{65: 'Ariz.', 73: 'Arizona', 67: 'Arizona'}], [{65: 'Ark.', 73: 'Arkansas', 67: 'Arkansas'}], [{65: 'Aug.', 73: 'August', 67: 'August'}], [{65: 'Calif.', 73: 'California', 67: 'California'}], [{65: 'Colo.', 73: 'Colorado', 67: 'Colorado'}], [{65: 'Conn.', 73: 'Connecticut', 67: 'Connecticut'}], [{65: 'Dec.', 73: 'December', 67: 'December'}], [{65: 'Del.', 73: 'Delaware', 67: 'Delaware'}], [{65: 'Feb.', 73: 'February', 67: 'February'}], [{65: 'Fla.', 73: 'Florida', 67: 'Florida'}], [{65: 'Ga.', 73: 'Georgia', 67: 'Georgia'}], [{65: 'Ia.', 73: 'Iowa', 67: 'Iowa'}], [{65: 'Id.', 73: 'Idaho', 67: 'Idaho'}], [{65: 'Ill.', 73: 'Illinois', 67: 'Illinois'}], [{65: 'Ind.', 73: 'Indiana', 67: 'Indiana'}], [{65: 'Jan.', 73: 'January', 67: 'January'}], [{65: 'Jul.', 73: 'July', 67: 'July'}], [{65: 'Jun.', 73: 'June', 67: 'June'}], [{65: 'Kan.', 73: 'Kansas', 67: 'Kansas'}], [{65: 'Kans.', 73: 'Kansas', 67: 'Kansas'}], [{65: 'Ky.', 73: 'Kentucky', 67: 'Kentucky'}], [{65: 'La.', 73: 'Louisiana', 67: 'Louisiana'}], [{65: 'Mar.', 73: 'March', 67: 'March'}], [{65: 'Mass.', 73: 'Massachusetts', 67: 'Massachusetts'}], [{65: 'May.', 73: 'May', 67: 'May'}], [{65: 'Mich.', 73: 'Michigan', 67: 'Michigan'}], [{65: 'Minn.', 73: 'Minnesota', 67: 'Minnesota'}], [{65: 'Miss.', 73: 'Mississippi', 67: 'Mississippi'}], [{65: 'N.C.', 73: 'North Carolina', 67: 'North Carolina'}], [{65: 'N.D.', 73: 'North Dakota', 67: 'North Dakota'}], [{65: 'N.H.', 73: 'New Hampshire', 67: 'New Hampshire'}], [{65: 'N.J.', 73: 'New Jersey', 67: 'New Jersey'}], [{65: 'N.M.', 73: 'New Mexico', 67: 'New Mexico'}], [{65: 'N.Y.', 73: 'New York', 67: 'New York'}], [{65: 'Neb.', 73: 'Nebraska', 67: 'Nebraska'}], [{65: 'Nebr.', 73: 'Nebraska', 67: 'Nebraska'}], [{65: 'Nev.', 73: 'Nevada', 67: 'Nevada'}], [{65: 'Nov.', 73: 'November', 67: 'November'}], [{65: 'Oct.', 73: 'October', 67: 'October'}], [{65: 'Okla.', 73: 'Oklahoma', 67: 'Oklahoma'}], [{65: 'Ore.', 73: 'Oregon', 67: 'Oregon'}], [{65: 'Pa.', 73: 'Pennsylvania', 67: 'Pennsylvania'}], [{65: 'S.C.', 73: 'South Carolina', 67: 'South Carolina'}], [{65: 'Sep.', 73: 'September', 67: 'September'}], [{65: 'Sept.', 73: 'September', 67: 'September'}], [{65: 'Tenn.', 73: 'Tennessee', 67: 'Tennessee'}], [{65: 'Va.', 73: 'Virginia', 67: 'Virginia'}], [{65: 'Wash.', 73: 'Washington', 67: 'Washington'}], [{65: 'Wis.', 73: 'Wisconsin', 67: 'Wisconsin'}], [{65: \"'d\"}], [{65: 'a.m.'}], [{65: 'Adm.'}], [{65: 'Bros.'}], [{65: 'co.'}], [{65: 'Co.'}], [{65: 'Corp.'}], [{65: 'D.C.'}], [{65: 'Dr.'}], [{65: 'e.g.'}], [{65: 'E.g.'}], [{65: 'E.G.'}], [{65: 'Gen.'}], [{65: 'Gov.'}], [{65: 'i.e.'}], [{65: 'I.e.'}], [{65: 'I.E.'}], [{65: 'Inc.'}], [{65: 'Jr.'}], [{65: 'Ltd.'}], [{65: 'Md.'}], [{65: 'Messrs.'}], [{65: 'Mo.'}], [{65: 'Mont.'}], [{65: 'Mr.'}], [{65: 'Mrs.'}], [{65: 'Ms.'}], [{65: 'p.m.'}], [{65: 'Ph.D.'}], [{65: 'Prof.'}], [{65: 'Rep.'}], [{65: 'Rev.'}], [{65: 'Sen.'}], [{65: 'St.'}], [{65: 'vs.'}], [{65: 'v.s.'}]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"from spacy.lang.en.tokenizer_exceptions import TOKENIZER_EXCEPTIONS\\n\\nTOKENIZER_EXCEPTIONS.values()\";\n",
       "                var nbb_formatted_code = \"from spacy.lang.en.tokenizer_exceptions import TOKENIZER_EXCEPTIONS\\n\\nTOKENIZER_EXCEPTIONS.values()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.lang.en.tokenizer_exceptions import TOKENIZER_EXCEPTIONS\n",
    "\n",
    "TOKENIZER_EXCEPTIONS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excess-rebound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.109723Z",
     "start_time": "2021-03-28T20:32:44.104174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "$\n",
      "STOCK.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"tokens = tokenizer(\\\"This is a $STOCK.\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"tokens = tokenizer(\\\"This is a $STOCK.\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-adaptation",
   "metadata": {},
   "source": [
    "You can add special prefixes in the form of regex by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlikely-croatia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.115930Z",
     "start_time": "2021-03-28T20:32:44.111815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"custom_prefixes = nlp.Defaults.prefixes + (r\\\"\\\\$[a-zA-Z]+\\\",)\";\n",
       "                var nbb_formatted_code = \"custom_prefixes = nlp.Defaults.prefixes + (r\\\"\\\\$[a-zA-Z]+\\\",)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_prefixes = nlp.Defaults.prefixes + (r\"\\$[a-zA-Z]+\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "strategic-slovakia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.124436Z",
     "start_time": "2021-03-28T20:32:44.117070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"prefix_re = spacy.util.compile_prefix_regex(custom_prefixes)\";\n",
       "                var nbb_formatted_code = \"prefix_re = spacy.util.compile_prefix_regex(custom_prefixes)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(custom_prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "arabic-arena",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.134261Z",
     "start_time": "2021-03-28T20:32:44.125454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "$STOCK\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"import re\\n\\nprefix_re = re.compile(r\\\"\\\\$[a-zA-Z]+\\\")\\ntokenizer = Tokenizer(\\n    nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\\n)\\n\\ntokens = tokenizer(\\\"This is a $STOCK.\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"import re\\n\\nprefix_re = re.compile(r\\\"\\\\$[a-zA-Z]+\\\")\\ntokenizer = Tokenizer(\\n    nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\\n)\\n\\ntokens = tokenizer(\\\"This is a $STOCK.\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "prefix_re = re.compile(r\"\\$[a-zA-Z]+\")\n",
    "tokenizer = Tokenizer(\n",
    "    nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")\n",
    "\n",
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-breed",
   "metadata": {},
   "source": [
    "You can add also special-case tokenization rules. This mechanism is also used to add custom tokenizer exceptions to the language data. See the usage guide on the [languages data](https://spacy.io/usage/linguistic-features#language-data) and [tokenizer special cases](https://spacy.io/usage/linguistic-features#special-cases) for more details and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dirty-chester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.146952Z",
     "start_time": "2021-03-28T20:32:44.135427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "!\n",
      "give\n",
      "me\n",
      "five\n",
      "!\n",
      "you\n",
      "do\n",
      "not\n",
      "do\n",
      "that\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"from spacy.attrs import ORTH, NORM, LOWER\\n\\ndont_case = [{ORTH: \\\"do\\\"}, {ORTH: \\\"n't\\\", NORM: \\\"not\\\"}]\\ngimme_case = [{ORTH: \\\"gi\\\", NORM:\\\"give\\\"}, {ORTH: \\\"me\\\", NORM: \\\"me\\\"}]\\ntokenizer.add_special_case(\\\"don't\\\", dont_case)\\ntokenizer.add_special_case(\\\"gimme\\\", gimme_case)\\ntokens = tokenizer(\\\"Yo! gimme five!\\\")\\nfor token in tokens:\\n    print(token.norm_)\\ntokens = tokenizer(\\\"You don't do that\\\")\\nfor token in tokens:\\n    print(token.norm_)\";\n",
       "                var nbb_formatted_code = \"from spacy.attrs import ORTH, NORM, LOWER\\n\\ndont_case = [{ORTH: \\\"do\\\"}, {ORTH: \\\"n't\\\", NORM: \\\"not\\\"}]\\ngimme_case = [{ORTH: \\\"gi\\\", NORM: \\\"give\\\"}, {ORTH: \\\"me\\\", NORM: \\\"me\\\"}]\\ntokenizer.add_special_case(\\\"don't\\\", dont_case)\\ntokenizer.add_special_case(\\\"gimme\\\", gimme_case)\\ntokens = tokenizer(\\\"Yo! gimme five!\\\")\\nfor token in tokens:\\n    print(token.norm_)\\ntokens = tokenizer(\\\"You don't do that\\\")\\nfor token in tokens:\\n    print(token.norm_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.attrs import ORTH, NORM, LOWER\n",
    "\n",
    "dont_case = [{ORTH: \"do\"}, {ORTH: \"n't\", NORM: \"not\"}]\n",
    "gimme_case = [{ORTH: \"gi\", NORM:\"give\"}, {ORTH: \"me\", NORM: \"me\"}]\n",
    "tokenizer.add_special_case(\"don't\", dont_case)\n",
    "tokenizer.add_special_case(\"gimme\", gimme_case)\n",
    "tokens = tokenizer(\"Yo! gimme five!\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n",
    "tokens = tokenizer(\"You don't do that\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-checklist",
   "metadata": {},
   "source": [
    "When you load a model with pretrained NER (Named Entity Recognition), like `en_core_web_sm`, it is possible to make the tokenizer to merge the token for the entities it finds. Let's check what is inside the pipeline performed by `nlp`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "undefined-jesus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.151274Z",
     "start_time": "2021-03-28T20:32:44.148001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"nlp.pipeline\";\n",
       "                var nbb_formatted_code = \"nlp.pipeline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-consumer",
   "metadata": {},
   "source": [
    "There's a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sufficient-radar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.161196Z",
     "start_time": "2021-03-28T20:32:44.152194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Apple is a $1000b company.\\\")\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Apple is a $1000b company.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mighty-twenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.166456Z",
     "start_time": "2021-03-28T20:32:44.162183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "a\n",
      "$\n",
      "1000b\n",
      "company\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"for token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"for token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "convenient-census",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "1000b MONEY\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_formatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pregnant-working",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.200653Z",
     "start_time": "2021-03-28T20:32:44.178705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "Strive\n",
      "School\n",
      ".\n",
      "It\n",
      "'s\n",
      "worthy\n",
      "to\n",
      "merge\n",
      "'\n",
      "Strive\n",
      "School\n",
      "'\n",
      "as\n",
      "a\n",
      "single\n",
      "token\n",
      "instead\n",
      "of\n",
      "two\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sweet-mandate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.207796Z",
     "start_time": "2021-03-28T20:32:44.203763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strive School ORG\n",
      "Strive School' ORG\n",
      "two CARDINAL\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_formatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-calculator",
   "metadata": {},
   "source": [
    "Let's add \"merge_entities\" to the pipeline (you can do it only if there is the entity recognizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "laughing-military",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.212102Z",
     "start_time": "2021-03-28T20:32:44.208805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\";\n",
       "                var nbb_formatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "standing-textbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.216239Z",
     "start_time": "2021-03-28T20:32:44.213031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>),\n",
       " ('merge_entities', <function spacy.pipeline.functions.merge_entities(doc)>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"nlp.pipeline\";\n",
       "                var nbb_formatted_code = \"nlp.pipeline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "determined-montana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.233121Z",
     "start_time": "2021-03-28T20:32:44.217083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "Strive School\n",
      ".\n",
      "It\n",
      "'s\n",
      "worthy\n",
      "to\n",
      "merge\n",
      "'\n",
      "Strive School'\n",
      "as\n",
      "a\n",
      "single\n",
      "token\n",
      "instead\n",
      "of\n",
      "two\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stretch-solution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.237660Z",
     "start_time": "2021-03-28T20:32:44.234118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"TEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\";\n",
       "                var nbb_formatted_code = \"TEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "informal-petite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.257141Z",
     "start_time": "2021-03-28T20:32:44.238468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net\n",
      "income\n",
      "was\n",
      "$9.4 million\n",
      "compared\n",
      "to\n",
      "the prior year\n",
      "of\n",
      "$2.7 million\n",
      ".\n",
      "------------------\n",
      "Revenue\n",
      "exceeded\n",
      "twelve billion dollars\n",
      ",\n",
      "with\n",
      "a\n",
      "loss\n",
      "of\n",
      "$\n",
      "1b\n",
      ".\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"for sentence in nlp.pipe(TEXTS):\\n    for token in sentence:\\n        print(token)\\n    print(\\\"------------------\\\")\";\n",
       "                var nbb_formatted_code = \"for sentence in nlp.pipe(TEXTS):\\n    for token in sentence:\\n        print(token)\\n    print(\\\"------------------\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sentence in nlp.pipe(TEXTS):\n",
    "    for token in sentence:\n",
    "        print(token)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-maximum",
   "metadata": {},
   "source": [
    "It's also possible to merge the noun chunks into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "possible-client",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.263274Z",
     "start_time": "2021-03-28T20:32:44.258469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_noun_chunks\\\"))\";\n",
       "                var nbb_formatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_noun_chunks\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "frozen-custody",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.268793Z",
     "start_time": "2021-03-28T20:32:44.264555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>),\n",
       " ('merge_entities', <function spacy.pipeline.functions.merge_entities(doc)>),\n",
       " ('merge_noun_chunks',\n",
       "  <function spacy.pipeline.functions.merge_noun_chunks(doc)>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"nlp.pipeline\";\n",
       "                var nbb_formatted_code = \"nlp.pipeline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "every-wedding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.282564Z",
     "start_time": "2021-03-28T20:32:44.269711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "I\n",
      "'m\n",
      "Antonio Marsella\n",
      ",\n",
      "nice\n",
      "to\n",
      "meet\n",
      "you\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Hello, I'm Antonio Marsella, nice to meet you.\\\")\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Hello, I'm Antonio Marsella, nice to meet you.\\\")\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Hello, I'm Antonio Marsella, nice to meet you.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-cassette",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-brief",
   "metadata": {},
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "going-palace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['front', 'within', 'down', 'besides', 'each', 'until', 'more', 'take', 'have', 'various']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n",
       "                var nbb_formatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-tokyo",
   "metadata": {},
   "source": [
    "To remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "crucial-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determined\n",
      "drop\n",
      "his litigation\n",
      "the monastry\n",
      ",\n",
      "relinguish\n",
      "his claims\n",
      "wood\n",
      "-\n",
      "cuting\n",
      "\n",
      "\n",
      "fishery rihgts\n",
      ".\n",
      "ready\n",
      "this becuase\n",
      "the rights\n",
      "valuable\n",
      ",\n",
      "\n",
      "\n",
      "indeed the vaguest idea\n",
      "the wood\n",
      "river\n",
      "question\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-death",
   "metadata": {},
   "source": [
    "For adding customized stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acknowledged-bahamas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n",
       "                var nbb_formatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-scene",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "worthy-toilet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['determine',\n",
       " 'drop',\n",
       " 'litigation',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'relinguish',\n",
       " 'claim',\n",
       " 'wood',\n",
       " '-',\n",
       " 'cut',\n",
       " '\\n',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " '.',\n",
       " 'ready',\n",
       " 'becuase',\n",
       " 'right',\n",
       " 'valuable',\n",
       " ',',\n",
       " '\\n',\n",
       " 'vague',\n",
       " 'idea',\n",
       " 'wood',\n",
       " 'river',\n",
       " 'question',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n",
       "                var nbb_formatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "# not using merge_chunk_nouns\n",
    "doc = nlp(\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    ")\n",
    "\n",
    "lemma_word1 = []\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-large",
   "metadata": {},
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tested-lafayette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He determined to drop his litigation with the monastry and relinguish his claims to the woodcuting and \\nfishery rihgts at once He was the more ready to do this becuase the rights had become much less valuable and he had \\nindeed the vaguest idea where the wood and river in question were'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n",
       "                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "institutional-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "determined\n",
      "to\n",
      "drop\n",
      "his\n",
      "litigation\n",
      "with\n",
      "the\n",
      "monastry\n",
      "and\n",
      "relinguish\n",
      "his\n",
      "claims\n",
      "to\n",
      "the\n",
      "woodcuting\n",
      "and\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      "at\n",
      "once\n",
      "He\n",
      "was\n",
      "the\n",
      "more\n",
      "ready\n",
      "to\n",
      "do\n",
      "this\n",
      "becuase\n",
      "the\n",
      "rights\n",
      "had\n",
      "become\n",
      "much\n",
      "less\n",
      "valuable\n",
      "and\n",
      "he\n",
      "had\n",
      "\n",
      "\n",
      "indeed\n",
      "the\n",
      "vaguest\n",
      "idea\n",
      "where\n",
      "the\n",
      "wood\n",
      "and\n",
      "river\n",
      "in\n",
      "question\n",
      "were\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(text_no_punct)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-source",
   "metadata": {},
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "greater-cabin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heLo how are you?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 99;\n",
       "                var nbb_unformatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n",
       "                var nbb_formatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "monthly-rapid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 100;\n",
       "                var nbb_unformatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n",
       "                var nbb_formatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "text = nlp(text)\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown([token.text for token in text])\n",
    "\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-marriage",
   "metadata": {},
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "terminal-occupation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hero how are you?'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n",
       "                var nbb_formatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "spell(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-language",
   "metadata": {},
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-dressing",
   "metadata": {},
   "source": [
    "**For spacy before v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "false-portrait",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study']\n",
      "['studying']\n",
      "['studying']\n",
      "['study']\n",
      "['studying']\n",
      "['studying']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"studying\", VERB))\n",
    "print(lemmatizer(\"studying\", NOUN))\n",
    "print(lemmatizer(\"studying\", ADJ))\n",
    "\n",
    "# or as alternative\n",
    "\n",
    "print(lemmatizer.verb(\"studying\"))\n",
    "print(lemmatizer.noun(\"studying\"))\n",
    "print(lemmatizer.adj(\"studying\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-header",
   "metadata": {},
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-showcase",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-chair",
   "metadata": {},
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "literary-beads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "VBZ\n",
      "VBG\n",
      "NNP\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-arrow",
   "metadata": {},
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swiss-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, proper singular\n",
      "verb, 3rd person singular present\n",
      "verb, gerund or present participle\n",
      "noun, proper singular\n",
      "conjunction, subordinating or preposition\n",
      "noun, proper singular\n",
      "noun, proper singular\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-mercy",
   "metadata": {},
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worst-bracelet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio      PROPN      NNP      noun, proper singular\n",
      "is           AUX        VBZ      verb, 3rd person singular present\n",
      "learning     VERB       VBG      verb, gerund or present participle\n",
      "Python       PROPN      NNP      noun, proper singular\n",
      "in           ADP        IN       conjunction, subordinating or preposition\n",
      "Strive       PROPN      NNP      noun, proper singular\n",
      "School       PROPN      NNP      noun, proper singular\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-canberra",
   "metadata": {},
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-greek",
   "metadata": {},
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valuable-cambodia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 4, 87: 1, 100: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\n",
    "\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-selection",
   "metadata": {},
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "departmental-bermuda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.vocab[96].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-military",
   "metadata": {},
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cultural-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 stands for PROPN   : 4\n",
      "87 stands for AUX     : 1\n",
      "100 stands for VERB    : 1\n"
     ]
    }
   ],
   "source": [
    "for ID, frequency in num_pos.items():\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-chart",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "pacific-generation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "prescription-swimming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Antonio works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive School\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 131;\n",
       "                var nbb_unformatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_formatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "substantial-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "organized-reservoir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rome\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a big city.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 137;\n",
       "                var nbb_unformatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_formatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-gospel",
   "metadata": {},
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-soldier",
   "metadata": {},
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "removable-preserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sought-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Manchester United, Harry Kane, $90 million)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-hospital",
   "metadata": {},
   "source": [
    "We can access the entities text, label by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "foreign-lincoln",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United ORG\n",
      "Harry Kane PERSON\n",
      "$90 million MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-giant",
   "metadata": {},
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wound-nicaragua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United ORG Companies, agencies, institutions, etc.\n",
      "Harry Kane PERSON People, including fictional\n",
      "$90 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-artist",
   "metadata": {},
   "source": [
    "### Adding new entities\n",
    "\n",
    "If in a text an entity has not being identified correctly, you can manually add it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caring-requirement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:13.466172Z",
     "start_time": "2021-03-31T06:08:13.421585Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(u'Strive is setting up a new course.')\n",
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "neutral-cleaning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.759623Z",
     "start_time": "2021-03-31T06:08:15.749143Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "ORG = sentence.vocab.strings[u'ORG']\n",
    "\n",
    "new_entity = Span(sentence, 0, 1, label=ORG)\n",
    "sentence.ents = list(sentence.ents) + [new_entity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "surprising-token",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.930620Z",
     "start_time": "2021-03-31T06:08:15.922601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Strive,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-problem",
   "metadata": {},
   "source": [
    "First, we need to import the Span class from the `spacy.tokens` module. Next, we need to get the hash value of the ORG entity type from our document. After that, we need to assign the hash value of ORG to the span. Since \"Strive\" is the first word in the document, the span is 0-1. Finally, we need to add the new entity span to the list of entities. Now if you execute the following script, you will see \"Strive\" in the list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "anticipated-advantage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:10:27.513455Z",
     "start_time": "2021-03-31T06:10:27.498248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strive - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prospective-springer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is setting up a new course.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-trance",
   "metadata": {},
   "source": [
    "We can also filter which entity to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "wound-scenario",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " demand \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "permanent-buffer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter = {'ents': ['ORG']}\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-toronto",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-participant",
   "metadata": {},
   "source": [
    "The Matcher lets you find words and phrases using rules describing their token attributes. Rules can refer to token annotations (like the text or part-of-speech tags), as well as lexical attributes like Token.is_punct. Applying the matcher to a Doc gives you access to the matched tokens in context. For in-depth examples and workflows for combining rules and statistical models, see the usage guide on rule-based matching.\n",
    "\n",
    "https://spacy.io/api/matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-emphasis",
   "metadata": {},
   "source": [
    "Test the explorer: https://explosion.ai/demos/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-brake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:48.315906Z",
     "start_time": "2021-03-31T09:15:48.312447Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-works",
   "metadata": {},
   "source": [
    "\n",
    "Let’s say we want to enable spaCy to find a combination of three tokens:\n",
    "\n",
    "- A token whose lowercase form matches “hello”, e.g. “Hello” or “HELLO”.\n",
    "- A token whose is_punct flag is set to True, i.e. any punctuation.\n",
    "- A token whose lowercase form matches “world”, e.g. “World” or “WORLD”.\n",
    "\n",
    "The pattern would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "tired-biotechnology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:28:52.532218Z",
     "start_time": "2021-03-31T06:28:52.518505Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-moral",
   "metadata": {},
   "source": [
    "We can define a custom matcher that satisfy that pattern by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "forward-collar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:29:52.821170Z",
     "start_time": "2021-03-31T06:29:52.795421Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"HelloWorld\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "martial-savannah",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:30:16.443906Z",
     "start_time": "2021-03-31T06:30:16.415047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "streaming-snapshot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:33:07.773269Z",
     "start_time": "2021-03-31T06:33:07.763583Z"
    }
   },
   "outputs": [],
   "source": [
    "golang_pattern = [{\"LOWER\":{\n",
    "    \"IN\":[\"go\", \"golang\"]}, \n",
    "                   \"POS\":{\"NOT_IN\":[\"VERB\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "limited-breach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:07.310668Z",
     "start_time": "2021-03-31T06:36:07.279388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLANG 5 6 Go\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I go the learn the Go programming language\")\n",
    "matcher.add(\"GOLANG\", [golang_pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "damaged-storm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:12.070611Z",
     "start_time": "2021-03-31T06:36:12.066899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "VERB\n",
      "DET\n",
      "VERB\n",
      "DET\n",
      "PROPN\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alternative-belief",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:35.197670Z",
     "start_time": "2021-03-31T09:15:35.171259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This\n",
      "1 is\n",
      "2 a\n",
      "3 tweet\n",
      "4 about\n",
      "5 Dogecoin\n",
      "6 $\n",
      "7 DOGE\n",
      "8 #\n",
      "9 DOGE\n"
     ]
    }
   ],
   "source": [
    "text = nlp(\"This is a tweet about Dogecoin $DOGE #DOGE\")\n",
    "\n",
    "for i, token in enumerate(text):\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "necessary-fever",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:39.837461Z",
     "start_time": "2021-03-31T09:15:39.833204Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = [ {\"ORTH\":\"$\", \"OP\":\"?\"},\n",
    "           {\"ORTH\":\"#\", \"OP\":\"?\"},\n",
    "        {\"LOWER\": {\"IN\":[\"doge\", \"dogecoin\"]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "super-seminar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:51.783195Z",
     "start_time": "2021-03-31T09:15:51.779647Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOGE\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "several-contributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:52.142087Z",
     "start_time": "2021-03-31T09:15:52.131733Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE 5 6 Dogecoin\n",
      "DOGE 6 8 $DOGE\n",
      "DOGE 7 8 DOGE\n",
      "DOGE 8 10 #DOGE\n",
      "DOGE 9 10 DOGE\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-fight",
   "metadata": {},
   "source": [
    "You can also match \"whatever token\" by adding an empty \\{\\} in the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "future-buying",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.324240Z",
     "start_time": "2021-03-31T09:18:49.304322Z"
    }
   },
   "outputs": [],
   "source": [
    "text = nlp(\"I want to match whatever follows here\")\n",
    "pattern = [ {\"LOWER\":\"follows\"},\n",
    "             {}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "formed-irish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.728059Z",
     "start_time": "2021-03-31T09:18:49.725041Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"WHATEVER\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "monetary-utility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.890304Z",
     "start_time": "2021-03-31T09:18:49.882152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHATEVER 5 7 follows here\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-anime",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
