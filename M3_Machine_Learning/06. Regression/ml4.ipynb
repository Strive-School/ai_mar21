{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97ca96459c18e31e022142d562ddb09f",
     "grade": false,
     "grade_id": "cell-6bdba789962f4139",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Guided Notebook on Logistical Regression from Scratch. \n",
    "\n",
    "*Note: The original post can be found in [Nick Becker's Github](https://github.com/beckernick/logistic_regression_from_scratch)*\n",
    "\n",
    "In this post, we will walk through implementig basic binary logistic regression from scratch. Logistic regression is a generalized linear model that we can use to model or predict categorical outcome variables. We might use logistic regression to predict whether someone will be denied or approved for a loan, but probably not to predict the value of someone's house.\n",
    "\n",
    "So, how does it work? In logistic regression, we're essentially trying to find the weights that maximize the likelihood of producing our given data. Maximum Likelihood Estimation is a well covered topic in statistics courses (if you are interested to delve into the math and get high-level description, click [here](http://www2.stat.duke.edu/~banks/111-lectures.dir/lect10.pdf)), and it is extremely useful.\n",
    "\n",
    "Since this maximizing the likelihood is an iterative process, I'll solve the optimization problem with gradient descent. Before I do that, though, I need some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "883518036f38474846f08527b5c3a4e6",
     "grade": false,
     "grade_id": "cell-f878cbdd284742a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Generating Data\n",
    "As we saw last class, we can use simulated data. This can be done by sampling from a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef2478c1c4128a6b8cee468edc9aee8c",
     "grade": false,
     "grade_id": "cell-84bed59f2c1afe60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c745e423024fa7de3b431b669590a2a",
     "grade": false,
     "grade_id": "cell-334fa9dd52bfef34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Picking a Link Function\n",
    "Generalized linear models usually tranform a linear model of the predictors by using a [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function). In logistic regression, the link function is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). We can implement this really easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3ca0182459c51203a8ddaee93d7316f",
     "grade": false,
     "grade_id": "cell-7129eee06dd2e6f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "def sigmoid(scores):\n",
    "    # the sigmoid function is 1 / (1 + e^x)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b33a1b08ba03c6ab71d414409ec8b888",
     "grade": true,
     "grade_id": "cell-0370c5c0473068d3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sigmoid(np.array([0,1,0])).shape ==(3,)\n",
    "assert sigmoid(np.array([0,1,0]))[0] == 0.5\n",
    "assert abs(sigmoid(np.array([0,1,0]))[1] -0.73) <= 0.009\n",
    "assert sigmoid(np.array([100,0,0]))[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e315149343018e9b6d899bd855241db",
     "grade": false,
     "grade_id": "cell-2ccc04db38dbe11e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Maximizing the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23230f12d99e2a2d852611a78853e45c",
     "grade": false,
     "grade_id": "cell-0a1b63eccfbbe0d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To maximize the likelihood, I need a way to compute the likelihood and the gradient of the likelihood. Fortunately, the likelihood (for binary classification) can be reduced to a fairly intuitive form by switching to the log-likelihood. We're able to do this without affecting the weights parameter estimation because log transformation are [monotonic](https://en.wikipedia.org/wiki/Monotonic_function).\n",
    "\n",
    "For anyone interested in the derivations of the functions I'm using, check out Section 4.4.1 of Hastie, Tibsharani, and Friedman's [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/). For those less mathematically inclined, Carlos Guestrin (Univesity of Washington) details one possible derivation of the log-likelihood in a series of short lectures on [Coursera](https://www.coursera.org/learn/ml-classification/lecture/1ZeTC/very-optional-expressing-the-log-likelihood) using indicator functions.\n",
    "\n",
    "## Calculating the Log-Likelihood\n",
    "\n",
    "The log-likelihood can be viewed as as sum over all the training data. Mathematically,\n",
    "\n",
    "$$\\begin{equation}\n",
    "ll = \\sum_{i=1}^{N}y_{i}\\beta ^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})\n",
    "\\end{equation}$$\n",
    "\n",
    "where $y$ is the target class, $x_{i}$ represents an individual data point, and $\\beta$ is the weights vector.\n",
    "\n",
    "I can easily turn that into a function and take advantage of matrix algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495f9de41b18d5049b76ee302541700f",
     "grade": false,
     "grade_id": "cell-be66fa80d98053c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "def log_likelihood(features, target, weights):\n",
    "    # scores = features * weights\n",
    "    # ll = SUM{ target * scores - log( 1 + e^x)}\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "199a863552468dd7a2b3f8c42ae4bd49",
     "grade": true,
     "grade_id": "cell-9769a6c05e3be374",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert -8.1<= log_likelihood(np.array([1,0,1]), np.array([1,0,0]), np.array([5,-2,-1])) <= -8\n",
    "assert -2.4<= log_likelihood(np.array([0,1,0]), np.array([0,1,0]), np.array([5,-2,-1])) <= -2.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91795c9ca4ab3f3851e20ad41c0c634e",
     "grade": false,
     "grade_id": "cell-a52b6c3debaac8d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Calculating the Gradient\n",
    "\n",
    "Now I need an equation for the gradient of the log-likelihood. By taking the derivative of the equation above and reformulating in matrix form, the gradient becomes: \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\bigtriangledown ll = X^{T}(Y - Predictions)\n",
    "\\end{equation}$$\n",
    "\n",
    "Again, this is really easy to implement. It's so simple I don't even need to wrap it into a function. The gradient here looks very similar to the output layer gradient in a neural network that we will see later in the course.\n",
    "\n",
    "This shouldn't be too surprising, since a neural network is basically just a series of non-linear link functions applied after linear manipulations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0b963df525049394e6a7118fd97ac4b",
     "grade": false,
     "grade_id": "cell-cea663d039e6e54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Building the Logistic Regression Function\n",
    "\n",
    "Finally, I'm ready to build the model function. I'll add in the option to calculate the model with an intercept, since it's a good option to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d3a1ad7a42e7dc2b5970f776d422f13",
     "grade": false,
     "grade_id": "cell-7411cb89737efb29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 points\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # scores -> feature * weights\n",
    "        # preds -> sigmoid(scores)\n",
    "        # error = target-preds\n",
    "        # gradient -> transposed feature * error\n",
    "        # weight -> weights + learning_rate * gradient\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # Print log-likelihood every so often\n",
    "        if step % 10000 == 0:\n",
    "            print(log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83be4a123815e10a56f29a1a8340432",
     "grade": false,
     "grade_id": "cell-74ef1d48b93dfcbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Time to do the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37b2677aec480b24f88420370bed2914",
     "grade": true,
     "grade_id": "cell-c3f0985051816a55",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert -14 <= weights[0] <= -13\n",
    "assert -5.2 <= weights[1] <= -4.3\n",
    "assert 7 <= weights[2] <= 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5f616a6d179f4b4f0e97a5e972485b9",
     "grade": false,
     "grade_id": "cell-3e14e41c377d51f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Comparing to Sk-Learn's LogisticRegression\n",
    "How do I know if my algorithm spit out the right weights? Well, one the one hand, the math looks right -- so I should be confident it's correct.\n",
    "\n",
    "Fortunately, I can compare my functions' weights to the weights from sk-learn's logistic regression function, which is known to be a correct implementation. They should be the same if I did everything correctly. Since sk-learn's `LogisticRegression` automatically regularizes (which I didn't do), I set `C=1e15` to essentially turn off regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6616d5bd164c5340bd5a1f45fb7db5be",
     "grade": false,
     "grade_id": "cell-93e7e715ab59a636",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# 2 points\n",
    "\n",
    "# implement sklearn logistic regresion and fit\n",
    "# fit_intercept=True\n",
    "# C = 1e15\n",
    "clf = 0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f92f0c080b745dfc3c70e91d78bfa74b",
     "grade": true,
     "grade_id": "cell-bad0ca5b220f9d1f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert -14.5 <= clf.intercept_[0] <= -13\n",
    "assert -5.2 <= clf.coef_[0][0] <= -4.3\n",
    "assert 7 <= clf.coef_[0][1] <= 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25ed42b8f5ca82c936cd7adcd1a5aca7",
     "grade": false,
     "grade_id": "cell-fe6679f0a95af7bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As expected, my weights nearly perfectly match the sk-learn `LogisticRegression` weights. If I trained the algorithm longer and with a small enough learning rate, they would eventually match exactly. Why? Because gradient descent on a convex function will always reach the global optimum, given enough time and sufficiently small learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a451b933f30f4506735ac95ab582124d",
     "grade": false,
     "grade_id": "cell-b225d15fa9f005f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# What's the Accuracy?\n",
    "To get the accuracy, I just need to use the final weights to get the logits for the dataset (`final_scores`). Then I can use `sigmoid` to get the final predictions and round them to the nearest integer (0 or 1) to get the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85cb1d183bb2260760d495f8d4de0f40",
     "grade": false,
     "grade_id": "cell-d03109fc5a50911d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "#final scores =  np.ones(data.shape[0]) -> stack with data -> stacked_data * w\n",
    "#preds = round(sigmoid(inal scores ))\n",
    "# accuracy is the percentages of correct guesses 0-1\n",
    "\n",
    "accuracy = 0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print('Accuracy from scratch: {0}'.format((preds == simulated_labels).sum().astype(float) / len(preds)))\n",
    "print('Accuracy from sk-learn: {0}'.format(clf.score(simulated_separableish_features, simulated_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea616ec02589271d66242f5cc3360733",
     "grade": true,
     "grade_id": "cell-d5d0393eeaf89087",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert accuracy >= 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08a247d28ba76bfaa3ec701751b14397",
     "grade": false,
     "grade_id": "cell-d3bf5d90d9707c72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nearly perfect (which makes sense given the data). We should only have made mistakes right in the middle between the clusters. Let's make sure that's what happened. In the following plot, blue points are correct predictions, and red points are incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = preds == simulated_labels - 1, alpha = .8, s = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ee03134b4060266e927aab0f45d35df",
     "grade": false,
     "grade_id": "cell-0cdb35181d21919b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "887217b2cf4e9e089f8fc13f883ce9d2",
     "grade": false,
     "grade_id": "cell-d4b79088833e04a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this post, I built a logistic regression function from scratch and compared it with sk-learn's logistic regression function. While both functions give essentially the same result, my own function is **_significantly_** slower because it uses a highly optimized solver. While I'd probably never use my own algorithm in production, building algorithms from scratch makes it easier to think about how you could extend the algorithm to fit problems in new domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1202a258da0805ca47c7b0633bdd12d",
     "grade": false,
     "grade_id": "cell-5133e10dae62f23f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bonus Resources:\n",
    "\n",
    "- By the same author, [Neural Networks from Scratch](https://beckernick.github.io/neural-network-scratch/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
