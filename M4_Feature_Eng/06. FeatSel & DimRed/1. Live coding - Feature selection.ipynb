{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013476,
     "end_time": "2020-10-01T00:24:42.087877",
     "exception": false,
     "start_time": "2020-10-01T00:24:42.074401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Often you'll have hundreds or thousands of features after various encodings and feature generation. This can lead to two problems. First, the more features you have, the more likely you are to overfit to the training and validation sets. This will cause your model to perform worse at generalizing to new data.\n",
    "\n",
    "Secondly, the more features you have, the longer it will take to train your model and optimize hyperparameters. Also, when building user-facing products, you'll want to make inference as fast as possible. Using fewer features can speed up inference at the cost of predictive performance.\n",
    "\n",
    "To help with these issues, you'll want to use feature selection techniques to keep the most informative features for your model.\n",
    "\n",
    "We'll show that in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:42.171243Z",
     "iopub.status.busy": "2020-10-01T00:24:42.154124Z",
     "iopub.status.idle": "2020-10-01T00:24:48.766174Z",
     "shell.execute_reply": "2020-10-01T00:24:48.765408Z"
    },
    "papermill": {
     "duration": 6.665382,
     "end_time": "2020-10-01T00:24:48.766320",
     "exception": false,
     "start_time": "2020-10-01T00:24:42.100938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n",
    "                 parse_dates=['deadline', 'launched'])\n",
    "\n",
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "\n",
    "# Timestamp features\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "# Label encoding\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "\n",
    "data_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\n",
    "baseline_data = ks[data_cols].join(encoded)\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "interactions = pd.DataFrame(index=ks.index)\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "    # Convert to strings and combine\n",
    "    new_values = ks[col1].map(str) + \"_\" + ks[col2].map(str)\n",
    "    label_enc = LabelEncoder()\n",
    "    interactions[new_col_name] = label_enc.fit_transform(new_values)\n",
    "baseline_data = baseline_data.join(interactions)\n",
    "\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "\n",
    "baseline_data = baseline_data.join(count_7_days)\n",
    "\n",
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600.\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "timedeltas = timedeltas.fillna(timedeltas.max())\n",
    "\n",
    "baseline_data = baseline_data.join(timedeltas.rename({'launched': 'time_since_last_project'}, axis=1))\n",
    "\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    valid_fraction = 0.1\n",
    "    valid_size = int(len(dataframe) * valid_fraction)\n",
    "\n",
    "    train = dataframe[:-valid_size * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_size * 2:-valid_size]\n",
    "    test = dataframe[-valid_size:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid):\n",
    "    feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score:.4f}\")\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011681,
     "end_time": "2020-10-01T00:24:48.790363",
     "exception": false,
     "start_time": "2020-10-01T00:24:48.778682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Univariate Feature Selection\n",
    "\n",
    "The simplest and fastest methods are based on univariate statistical tests. For each feature, measure how strongly the target depends on the feature using a statistical test like $\\chi^2$ or ANOVA.\n",
    "\n",
    "From the scikit-learn feature selection module, `feature_selection.SelectKBest` returns the K best features given some scoring function. For our classification problem, the module provides three different scoring functions: $\\chi^2$, ANOVA F-value, and the mutual information score. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. The mutual information score is nonparametric and so can capture nonlinear relationships.\n",
    "\n",
    "With `SelectKBest`, we define the number of features to keep, based on the score from the scoring function. Using `.fit_transform(features, target)` we get back an array with only the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:48.822913Z",
     "iopub.status.busy": "2020-10-01T00:24:48.821938Z",
     "iopub.status.idle": "2020-10-01T00:24:49.277504Z",
     "shell.execute_reply": "2020-10-01T00:24:49.276828Z"
    },
    "papermill": {
     "duration": 0.475256,
     "end_time": "2020-10-01T00:24:49.277657",
     "exception": false,
     "start_time": "2020-10-01T00:24:48.802401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2015.,    5.,    9.,   18., 1409.],\n",
       "       [2017.,   13.,   22.,   31.,  957.],\n",
       "       [2013.,   13.,   22.,   31.,  739.],\n",
       "       ...,\n",
       "       [2010.,   13.,   22.,   31.,  238.],\n",
       "       [2016.,   13.,   22.,   31., 1100.],\n",
       "       [2011.,   13.,   22.,   31.,  542.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(baseline_data[feature_cols], baseline_data['outcome'])\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012452,
     "end_time": "2020-10-01T00:24:49.303412",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.290960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "However, I've done something wrong here. The statistical tests are calculated using all of the data. This means information from the validation and test sets could influence the features we keep, introducing a source of leakage. This means we should select features using only a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:49.351340Z",
     "iopub.status.busy": "2020-10-01T00:24:49.350505Z",
     "iopub.status.idle": "2020-10-01T00:24:49.460300Z",
     "shell.execute_reply": "2020-10-01T00:24:49.459609Z"
    },
    "papermill": {
     "duration": 0.144022,
     "end_time": "2020-10-01T00:24:49.460448",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.316426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.015e+03, 5.000e+00, 9.000e+00, 1.800e+01, 1.409e+03],\n",
       "       [2.017e+03, 1.300e+01, 2.200e+01, 3.100e+01, 9.570e+02],\n",
       "       [2.013e+03, 1.300e+01, 2.200e+01, 3.100e+01, 7.390e+02],\n",
       "       ...,\n",
       "       [2.011e+03, 1.300e+01, 2.200e+01, 3.100e+01, 5.150e+02],\n",
       "       [2.015e+03, 1.000e+00, 3.000e+00, 2.000e+00, 1.306e+03],\n",
       "       [2.013e+03, 1.300e+01, 2.200e+01, 3.100e+01, 1.084e+03]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(train[feature_cols], train['outcome'])\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015044,
     "end_time": "2020-10-01T00:24:49.489175",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.474131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You should notice that the selected features are different than when I used the entire dataset. Now we have our selected features, but it's only the feature values for the training set. To drop the rejected features from the validation and test sets, we need to figure out which columns in the dataset were kept with `SelectKBest`. To do this, we can use `.inverse_transform` to get back an array with the shape of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:49.525269Z",
     "iopub.status.busy": "2020-10-01T00:24:49.523282Z",
     "iopub.status.idle": "2020-10-01T00:24:49.570639Z",
     "shell.execute_reply": "2020-10-01T00:24:49.569983Z"
    },
    "papermill": {
     "duration": 0.068063,
     "end_time": "2020-10-01T00:24:49.570780",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.502717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>category</th>\n",
       "      <th>currency</th>\n",
       "      <th>country</th>\n",
       "      <th>category_currency</th>\n",
       "      <th>category_country</th>\n",
       "      <th>currency_country</th>\n",
       "      <th>count_7_days</th>\n",
       "      <th>time_since_last_project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>957.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   goal  hour  day  month    year  category  currency  country  \\\n",
       "0   0.0   0.0  0.0    0.0  2015.0       0.0       5.0      9.0   \n",
       "1   0.0   0.0  0.0    0.0  2017.0       0.0      13.0     22.0   \n",
       "2   0.0   0.0  0.0    0.0  2013.0       0.0      13.0     22.0   \n",
       "3   0.0   0.0  0.0    0.0  2012.0       0.0      13.0     22.0   \n",
       "4   0.0   0.0  0.0    0.0  2015.0       0.0      13.0     22.0   \n",
       "\n",
       "   category_currency  category_country  currency_country  count_7_days  \\\n",
       "0                0.0               0.0              18.0        1409.0   \n",
       "1                0.0               0.0              31.0         957.0   \n",
       "2                0.0               0.0              31.0         739.0   \n",
       "3                0.0               0.0              31.0         907.0   \n",
       "4                0.0               0.0              31.0        1429.0   \n",
       "\n",
       "   time_since_last_project  \n",
       "0                      0.0  \n",
       "1                      0.0  \n",
       "2                      0.0  \n",
       "3                      0.0  \n",
       "4                      0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014145,
     "end_time": "2020-10-01T00:24:49.599362",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.585217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This returns a DataFrame with the same index and columns as the training set, but all the dropped columns are filled with zeros. We can find the selected columns by choosing features where the variance is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:49.635029Z",
     "iopub.status.busy": "2020-10-01T00:24:49.634201Z",
     "iopub.status.idle": "2020-10-01T00:24:49.706831Z",
     "shell.execute_reply": "2020-10-01T00:24:49.706195Z"
    },
    "papermill": {
     "duration": 0.093457,
     "end_time": "2020-10-01T00:24:49.706975",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.613518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>currency</th>\n",
       "      <th>country</th>\n",
       "      <th>currency_country</th>\n",
       "      <th>count_7_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302896</th>\n",
       "      <td>2015</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>1534.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302897</th>\n",
       "      <td>2013</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302898</th>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>851.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302899</th>\n",
       "      <td>2014</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>1973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302900</th>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2163.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  currency  country  currency_country  count_7_days\n",
       "302896  2015        13       22                31        1534.0\n",
       "302897  2013        13       22                31         625.0\n",
       "302898  2014         5        9                18         851.0\n",
       "302899  2014        13       22                31        1973.0\n",
       "302900  2014         5        9                18        2163.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "# Get the valid dataset with the selected features.\n",
    "valid[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02915,
     "end_time": "2020-10-01T00:24:49.751708",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.722558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# L1 regularization\n",
    "\n",
    "Univariate methods consider only one feature at a time when making a selection decision. Instead, we can make our selection using all of the features by including them in a linear model with L1 regularization. This type of regularization (sometimes called Lasso) penalizes the absolute magnitude of the coefficients, as compared to L2 (Ridge) regression which penalizes the square of the coefficients.\n",
    "\n",
    "As the strength of regularization is increased, features which are less important for predicting the target are set to 0. This allows us to perform feature selection by adjusting the regularization parameter. We choose the parameter by finding the best performance on a hold-out set, or decide ahead of time how many features to keep.\n",
    "\n",
    "For regression problems you can use `sklearn.linear_model.Lasso`, or `sklearn.linear_model.LogisticRegression` for classification. These can be used along with `sklearn.feature_selection.SelectFromModel` to select the non-zero coefficients. Otherwise, the code is similar to the univariate tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:49.842268Z",
     "iopub.status.busy": "2020-10-01T00:24:49.837596Z",
     "iopub.status.idle": "2020-10-01T00:24:54.716269Z",
     "shell.execute_reply": "2020-10-01T00:24:54.715627Z"
    },
    "papermill": {
     "duration": 4.918552,
     "end_time": "2020-10-01T00:24:54.716412",
     "exception": false,
     "start_time": "2020-10-01T00:24:49.797860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+03, 1.200e+01, 1.100e+01, ..., 1.900e+03, 1.800e+01,\n",
       "        1.409e+03],\n",
       "       [3.000e+04, 4.000e+00, 2.000e+00, ..., 1.630e+03, 3.100e+01,\n",
       "        9.570e+02],\n",
       "       [4.500e+04, 0.000e+00, 1.200e+01, ..., 1.630e+03, 3.100e+01,\n",
       "        7.390e+02],\n",
       "       ...,\n",
       "       [2.500e+03, 0.000e+00, 3.000e+00, ..., 1.830e+03, 3.100e+01,\n",
       "        5.150e+02],\n",
       "       [2.600e+03, 2.100e+01, 2.300e+01, ..., 1.036e+03, 2.000e+00,\n",
       "        1.306e+03],\n",
       "       [2.000e+04, 1.600e+01, 4.000e+00, ..., 9.200e+02, 3.100e+01,\n",
       "        1.084e+03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "X, y = train[train.columns.drop(\"outcome\")], train['outcome']\n",
    "\n",
    "# Set the regularization parameter C=1\n",
    "logistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015971,
     "end_time": "2020-10-01T00:24:54.748761",
     "exception": false,
     "start_time": "2020-10-01T00:24:54.732790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Similar to the univariate tests, we get back an array with the selected features. Again, we will want to convert these to a DataFrame so we can get the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:54.789477Z",
     "iopub.status.busy": "2020-10-01T00:24:54.788042Z",
     "iopub.status.idle": "2020-10-01T00:24:54.926255Z",
     "shell.execute_reply": "2020-10-01T00:24:54.925403Z"
    },
    "papermill": {
     "duration": 0.161374,
     "end_time": "2020-10-01T00:24:54.926409",
     "exception": false,
     "start_time": "2020-10-01T00:24:54.765035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X.index,\n",
    "                                 columns=X.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015963,
     "end_time": "2020-10-01T00:24:54.959862",
     "exception": false,
     "start_time": "2020-10-01T00:24:54.943899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this case with the L1 parameter `C=1`, we're dropping the `time_since_last_project` column.\n",
    "\n",
    "In general, feature selection with L1 regularization is more powerful the univariate tests, but it can also be very slow when you have a lot of data and a lot of features. Univariate tests will be much faster on large datasets, but also will likely perform worse.\n",
    "\n",
    "# Your Turn\n",
    "Do **[feature selection](https://www.kaggle.com/kernels/fork/5407500)** to find the most important features in the model you've built in previous lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0157,
     "end_time": "2020-10-01T00:24:54.992319",
     "exception": false,
     "start_time": "2020-10-01T00:24:54.976619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161443) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 18.351231,
   "end_time": "2020-10-01T00:24:55.118356",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-01T00:24:36.767125",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
