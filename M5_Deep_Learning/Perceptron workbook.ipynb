{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the **neuron** is the most fundamental unit of the brain, the **perceptron** can be considered the most fundamental unit of a deep neural network. Both the neuron and the perceptron, also called *artificial neuron*, take an input, process it, pass it through an activation function that returns the activated output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/max/700/1*Fyapb-JRFJ-VtnLYLLXCwg.png)\n",
    "The Perceptron model is like the one showed in the image: first you have several inputs ($x_{1}, x_{2}, ..., x_{n}$). You can think of them like the features of the data you have worked on till now. Do you remember the *wonderful features* of the build week you worked on without knowing what they stand for? Those anonymous columns? Yes, like those.\n",
    "\n",
    "You investigated them trying to see the \"importance\" of each one using the correlation with the target, or the *shap* values. \n",
    "Well, a neuron try to figure out the importance of each input feature by itself. How?\n",
    "\n",
    "Look at those $w_{1}, w_{2},..., w_{n}$: they are the *weigths* that are associated with each input entry. As you can see on the right part of the picture, each $x_{i}$ is multiplied by the corresponding $w_{i}$ and then summed together. Those weights are not there to indicate the \"importance\" of each input feature, but instead the goal is to build a binary classifier, as you can expect from the 0 and 1 in the picture above. \n",
    "\n",
    "Ok, restart. The goal of a perceptron is to build a binary classifier, meaning a classifier that outputs either 0 or 1. In the case of a neuron, it \"spikes\" electricity through the axons, or doesn't. \n",
    "\n",
    "For this reason, the $w_{i}$s are the weights, either positive or negative, such that the weighted sum $\\sum_{i}w_{i}\\cdot x_{i}$ gives either 0 or 1. What does $b$ stand for? As you can expect, it's the bias component that you are used to already.\n",
    "\n",
    "I'll give you another example. Think that you want to decide either to watch a specific movie or not. You could have some \"features\" that you may consider before taking the decision. Let's say they are three:\n",
    "- $x_{1}$ is the feature: isAvengersMovie\n",
    "- $x_{2}$ is the feature: isRomanticMovie\n",
    "- $x_{3}$ is the feature: theMovieWonAnOscar\n",
    "\n",
    "What about the *bias*? Well, maybe a movie freak would watch all the movies he could, a more selective one would watch only Avengers movies. But at the end of the day, the weights and the bias will depend on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's assume that I'm that kind of guy who watches all the Avengers movies and hates romantic movies unless they won an Oscar.\n",
    "(It's not my case, I don't watch romantic movies as all... Ok, only when my gf forces me.).\n",
    "\n",
    "I chose binary variables for the sake of the example, but they can be any. \n",
    "\n",
    "Our training set looks like the following:\n",
    "- \"Titanic\", 1\n",
    "- \"Avengers: Endgame\", 1\n",
    "- \"I still believe\", 0\n",
    "- \"Spiderman\", 1\n",
    "\n",
    "Test set:\n",
    "- \"Chocolat\", 1\n",
    "\n",
    "where 1 stands for \"watched\", and 0 for \"not watched\". What we want to predict is if I will watch or not the movie \"Chocolat\".\n",
    "\n",
    "Let's represent the movies with the features above:\n",
    "\n",
    "|       title        | isAvengersMovie | isRomanticMovie | theMovieWonAnOscar | watched |\n",
    "|--------------------|-----------------|-----------------|--------------------|---------|\n",
    "| \"Titanic\"          | 0               | 1               | 1                  | 1       |\n",
    "| \"Avengers: Endgame\"| 1               | 0               | 0                  | 1       |\n",
    "| \"I still believe\"  | 0               | 1               | 0                  | 0       |\n",
    "| \"Spiderman\"        | 1               | 0               | 0                  | 1       |\n",
    "| \"Chocolat\"         | 0               | 1               | 1                  | 1       |\n",
    "\n",
    "Now the Machine Learning Scientist has to set the weights $w_{1}, w_{2}, w_{3}$. How would he start? One way, is to start randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3745401188473625 0.9507143064099162 0.7319939418114051\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w1, w2, w3 = np.random.rand(3)\n",
    "print(w1, w2, w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will change the sign and the scale of $w_{2}$ to have a more interesting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = -2*w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the vectors of features will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0, 1, 0, 1])\n",
    "x2 = np.array([1, 0, 1, 0])\n",
    "x3 = np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test (the movie \"Chocolat\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_test = 0\n",
    "x2_test = 1\n",
    "x3_test = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's missing? The bias $b$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3745401188473625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "b = np.random.rand(1)[0]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go. So let's start from the first sample, the \"Titanic\" movie. We have to compute:\n",
    "\n",
    "$x_{1}\\cdot w_{1} + x_{2}\\cdot w_{2} +  x_{3}\\cdot w_{3} + b$ \n",
    "\n",
    "Approximating $w_{1}, w_{2}, w_{3}, b$ to the third significant figure, and remembering that $x_{1} = 0, x_{2} = 1, x_{3}=1$, we have:\n",
    "$0.374\\cdot 0 - 1.901 \\cdot 1 + 0.732\\cdot 1 + 0.374 = - 0.795$.\n",
    "\n",
    "Now what? The result is neither 0 or 1! This is why we need an *activation function*! An activation function determines whether the neuron should be activated (â€œfiredâ€) or not, based on whether each neuron's input is relevant for the model's prediction. The most *basic* activation function you can think of is the one that gives you 1 in case of a positive outcome, 0 for 0 or negative one. In mathematical terms, if $g(\\mathbf{x})$ is the activation function given the input vector $\\mathbf{x}=(x_{1},x_{2}, x_{3})$\n",
    "\n",
    "$$ g(\\mathbf{x})=\\begin{cases}\n",
    "          1, & \\text{if}\\ \\mathbf{wx}+\\mathbf{b}>0 \\\\\n",
    "          0, & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "I used another notation here, just to make you flexible for different book notations. In some books, to distinguish scalars from vectors (*i.e.* simple numbers like 1, 1.5, -1 from (2, 1, 3.6)) it is used the bold for vectors. I recall you that $\\mathbf{wx}$ is the *dot product* between $\\mathbf{w}$ and $\\mathbf{x}$!\n",
    "\n",
    "Ok cool, what about the output of our algorithm for the \"Titanic\"? Since the output of $0.374\\cdot 0 - 1.901 \\cdot 1 + 0.732\\cdot 1 + 0.374 = - 0.795$ is negative, the activation function $g$ will make it 0.\n",
    "\n",
    "Oh, but this is a wrong prediction! We should adjust our weights!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap:\n",
    "\n",
    "- we randomly initialised the weights $\\mathbf{w}$ and the bias $\\mathbf{b}$\n",
    "- we used them to compute a value\n",
    "- we \"activated\" the value with the activation function $g$\n",
    "- we got the *wrong* prediction :(\n",
    "\n",
    "We learn from mistakes, right?\n",
    "\n",
    "Let me introduce you to the... Perceptron Learning Rule!\n",
    "Since the prediction was 0 instead of 1.\n",
    "What we wished for, was the dot product $\\mathbf{wx}$ to be higher than 0, because in this way the activation function would have returned 1! Since we got a negative value, we need to edit the weights a bit so that we are more likely to get a positive value for classifying this example and get the right score. \n",
    "\n",
    "Is it clear? I'll say it again: we predicted 0 instead of 1. We predicted 0 because $\\mathbf{wx}=-0.795$ and the activation function $g$ states that we say \"0\" if $\\mathbf{wx}$ is negative, so this is the case. If we want a positive outcome from $\\mathbf{wx}$, then we should increase a bit that value. How can we? We decided the old weights just randomly picking them, so can adjust them such that $\\mathbf{wx}$ for the \"Titanic\" example will be higher!\n",
    "\n",
    "Unpacking the dot product $\\mathbf{wx}$ for each component:\n",
    "\n",
    "- $w_{1}\\cdot x_{1}$ \n",
    "- $w_{2}\\cdot x_{2}$\n",
    "- $w_{2}\\cdot x_{3}$\n",
    "\n",
    "Since the $x_{i}$ are \"fixed\", they are given from the dataset, we can increase the $w_{i}$ like:\n",
    "\n",
    "$$w_{i(new)} = w_{i(old)} + (y-\\hat{y})x_{i}$$\n",
    "\n",
    "In this way: if we have no error, it means that $(y-\\hat{y})=0$, so we don't make any update. If we predicted 0 and the label was 1, then we have $1-0=1$ and $w_{i(new)}$ would become $w_{i(old)}+x_{i}$. If we predicted 1 and the label was 0, then $0-1= -1$ and $w_{i(new)}$ would become $w_{i(old)}-x_{i}$.\n",
    "\n",
    "In our example:\n",
    "\n",
    "- $w_{1(new)} = 0.374 + 0 = 0.374$\n",
    "- $w_{2(new)} = -1.901 + 1 = -0.901$\n",
    "- $w_{2(new)} = 0.732 + 1 = 1.732$\n",
    "- $b_{(new)} = 0.374 + 1 = 1.374$\n",
    "\n",
    "If we try the prediction again:\n",
    "\n",
    "$0.374\\cdot 0 - 0.901 \\cdot 1 + 1.732\\cdot 1 + 1.374 = 2.749$.\n",
    "\n",
    "*N.B.* To learn the bias as well, we add another component to the input and we consider it as 1. This i why you see \"+ 1\" in $b_{(new)} = 0.374 + 1 = 1.374$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 += x1[0]\n",
    "w2 += x2[0]\n",
    "w3 += x3[0]\n",
    "b += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2051054478389354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1*x1[0]+w2*x2[0]+w3*x3[0]+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a positive score this time. However, there's something more to point out. Learning in this way, will require a very big update of the weights at each iteration! This is why we introduce the:\n",
    "\n",
    "**Learning rate**\n",
    "\n",
    "that is the step size of the update for each iteration! The smaller the learning rate, the higher the number of iterations your algorithm will do, the bigger, the faster, but less accurate. If it is too big, sometimes you could overshoot too much.\n",
    "\n",
    "In this way, the update rule will be:\n",
    "\n",
    "$$w_{i(new)} = w_{i(old)} + \\eta(y-\\hat{y})x_{i}$$\n",
    "\n",
    "where $\\eta$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do with the rest of the samples? The idea is that we make a prediction for the first sample and update the weights, then with the second and we update the weights and then we restart from the first sample till we have classified correctly all the training set! Then, we are ready to test it on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap the algorithm:\n",
    "\n",
    "while not all the samples are correctly classified do:\n",
    "\n",
    "   - pick random $x\\in$ *TrainingSamples*\n",
    "     \n",
    "   - if $\\mathbf{wx}>0$ and the label of $x$ is 0 or viceversa:\n",
    "   \n",
    "       - update weights\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions that you could have so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What if it takes thousands of iterations to converge?\n",
    "\n",
    "Set a max iteration parameter to stop early. This allows you to avoid overfitting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What if the data are not linearly separable?\n",
    "\n",
    "Good point. Since we used a linear activation function, the algorithm converges only if there are linearly separable data. In the example below the algorithm would not converge ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-84d3404282e4>:7: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.axes().set_aspect('equal')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles, make_moons\n",
    "X_circle, y_circle = make_circles(100) # or try make_moons(100)\n",
    "y_circle = y_circle * 2 - 1\n",
    "X_circle*=4  # Make it a bit larger\n",
    "plt.scatter(X_circle[:,0], X_circle[:,1], c=y_circle, edgecolor='none')\n",
    "plt.xlim([-10,10]); plt.ylim([-10,10]); plt.grid()\n",
    "plt.axes().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How can we choose the learning rate?\n",
    "\n",
    "You can try different ones. Common choices are 0.1, 0.01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Could we see some code? \n",
    "\n",
    "Yes sure.\n",
    "\n",
    "We need:\n",
    "\n",
    "- labeled input data\n",
    "- a learning rate\n",
    "- some weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "input_data = [(np.array([0, 1, 1]), 1), (np.array([1, 0, 0]), 1),\n",
    "              (np.array([0, 1, 0]), 0), (np.array([1, 0, 0]), 1), \n",
    "              (np.array([0, 1, 1]), 1)]\n",
    "train = input_data[:-1]\n",
    "test = input_data[-1]\n",
    "weights = np.zeros(3)\n",
    "bias = 0\n",
    "max_iter = 10\n",
    "\n",
    "def perceptron(train, weights, bias, eta, max_iter):\n",
    "    it = 0\n",
    "    while it < max_iter: \n",
    "        new_weights = weights\n",
    "        new_bias = bias\n",
    "        for sample in train:\n",
    "            output = np.dot(weights, sample[0]) + bias \n",
    "            if output <= 0:\n",
    "                output = 0\n",
    "            else:\n",
    "                output = 1\n",
    "            if (sample[1] - output) != 0:\n",
    "                \n",
    "                weights += eta*(sample[1] - output)*sample[0]\n",
    "                bias += eta*(sample[1] - output)\n",
    "        if new_weights.all() == weights.all() and new_bias == bias:\n",
    "            print(f\"Convergence achieved at iteration {it}\")\n",
    "            break\n",
    "        it += 1\n",
    "        \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence achieved at iteration 2\n"
     ]
    }
   ],
   "source": [
    "weights, bias = perceptron(train, weights, bias, eta, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it one the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(weights, test[0])+bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is positive, so it will be classified as 1.\n",
    "What about the rest in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic predicted: 1 true label: 1\n",
      "Avengers: Endgame predicted: 1 true label: 1\n",
      "I still believe predicted: 0 true label: 0\n",
      "Spiderman predicted: 1 true label: 1\n"
     ]
    }
   ],
   "source": [
    "train_titles = [\"Titanic\", \"Avengers: Endgame\", \"I still believe\", \"Spiderman\"]\n",
    "for i, sample in enumerate(train):\n",
    "    output = np.dot(weights, sample[0]) + bias\n",
    "    print(train_titles[i], \"predicted:\", int(output>0), \"true label:\", sample[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is correct for all of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about non-linearly separable data? A classical example where the perceptron fails to converge to a solution that correctly classifies all the training data is the XOR problem.\n",
    "\n",
    "You know that the table truth of the XOR is:\n",
    "\n",
    "| A | B | A xor B |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "That is \"XOR is true if and only if ONLY one of the two statements is true\".\n",
    "\n",
    "![image](https://miro.medium.com/max/1400/1*E7YhKsJ2wb-VdeXpg89lvg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the picture, solving the equations for all the cases lead to an impossible solution:\n",
    "\n",
    "- Since $w_{0}$ is negative ($w_{0}<0$) it means that $-w_{0}$ is positive.\n",
    "- the 2nd and the 3rd lines says that both $w2$ and $w1$ are bigger than $-w_{0}$, so they are bigger than a positive number $\\rightarrow$ they are both positive\n",
    "- the sum of two positive number is obviously greater than the individuals (say 5+4 >5 and >4). So, the sum $w_{1}+w_{2}$ must be greater than $w_{1}$ and greater than $w_{2}$, and both should be greater than $-w_{0}$: the last row is a contraddiction with the 2nd and the 3rd!\n",
    "\n",
    "Let' test it with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = [(np.array([0,0]),0), (np.array([0,1]),1), (np.array([1,0]),1), (np.array([1,1]),0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence achieved at iteration 1\n"
     ]
    }
   ],
   "source": [
    "w_xor, b_xor = perceptron(xor, np.random.rand(2), bias=0, eta=0.1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 0 true label: 0\n",
      "predicted: 1 true label: 1\n",
      "predicted: 0 true label: 1\n",
      "predicted: 1 true label: 0\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(xor):\n",
    "    output = np.dot(w_xor, sample[0]) + b_xor\n",
    "    print(\"predicted:\", int(output>0), \"true label:\", sample[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, two of them have been misclassified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the activation function\n",
    "\n",
    "Till now we have used the step function $$ g(\\mathbf{x})=\\begin{cases}\n",
    "          1, & \\text{if}\\ \\mathbf{wx}+\\mathbf{b}>0 \\\\\n",
    "          0, & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "$$\n",
    "as activation function. Can we do better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe4a0cf7670>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.step([-1, 0,1], [0, 0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the picture above, we have a jump at 0. However, you may expect not so big difference between -0.001 and 0.001! As we did for the Logistic Regression, we may use the *sigmoid* or *logistic function* as activation function to have a smoother behaviour:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/max/1400/1*QLtPF0YkwYRLVoSeh2w4gw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, the output won't be either 0 or 1, but the probability (because we have a continuous number between 0 and 1) of 0 or 1!\n",
    "\n",
    "Let's implement the perceptron again but using the sigmoid this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def perceptron_with_sigmoid(train, weights, bias, eta, max_iter):\n",
    "    it = 0\n",
    "    while it < max_iter: \n",
    "        new_weights = weights\n",
    "        new_bias = bias\n",
    "        for sample in train:\n",
    "            output = np.dot(weights, sample[0]) + bias \n",
    "            ## I comment the old activation function\n",
    "            # if output <= 0:\n",
    "            #     output = 0\n",
    "            # else:\n",
    "            #     output = 1\n",
    "            ## New activation function\n",
    "            output = sigmoid(output)\n",
    "            if (sample[1] - output) != 0:\n",
    "                \n",
    "                weights += eta*(sample[1] - output)*sample[0]\n",
    "                bias += eta*(sample[1] - output)\n",
    "        if new_weights.all() == weights.all() and new_bias == bias:\n",
    "            print(f\"Convergence achieved at iteration {it}\")\n",
    "            break\n",
    "        it += 1\n",
    "        \n",
    "    return weights, bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see later why that having such a smooth and differentiable function helps neural network for computing the gradient of the loss function! \n",
    "\n",
    "## Conclusions\n",
    "\n",
    "We have seen the Perceptron algorithm, that is an algorithm to build a binary classifier for linearly separable data. We succesfully tested it on an example movies dataset, and unsuccesfully for the XOR problem, that contains not-linearly separable data. \n",
    "\n",
    "We learned that the Perceptron is for an artificial neural network what a neuron is for a biological one. We defined the sigmoid, a smooth and differentiable activation function that is often used in neural networks (you will meet cooler activation functions by the way!).\n",
    "\n",
    "For questions, feel free to ask! You can comment on Deepnote at this link: https://deepnote.com/project/59d3fdb3-682e-49b5-8cd1-f908f9765253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
